{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNVotsFgkCw74MJc9kw1Qqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/humb3rt84/UT/blob/main/MLDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the Required Data\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load ICUSTAYS dataset\n",
        "icustays_df = pd.read_csv('/content/drive/My Drive/MIMICfull/ICUSTAYS.csv')\n",
        "print(\"ICUSTAYS Columns:\")\n",
        "print(icustays_df.columns)\n",
        "print(icustays_df.head())\n",
        "\n",
        "# Load ADMISSIONS dataset\n",
        "admissions_df = pd.read_csv('/content/drive/My Drive/MIMICfull/ADMISSIONS.csv')\n",
        "print(\"ADMISSIONS Columns:\")\n",
        "print(admissions_df.columns)\n",
        "print(admissions_df.head())\n",
        "\n",
        "# Load NOTEEVENTS dataset (loading only a subset for initial exploration)\n",
        "noteevents_df = pd.read_csv('/content/drive/My Drive/MIMICfull/NOTEEVENTS.csv', low_memory=False)\n",
        "print(\"NOTEEVENTS Columns:\")\n",
        "print(noteevents_df.columns)\n",
        "print(noteevents_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srvkpj-bGC7h",
        "outputId": "6a0af6f0-bf0f-47ee-e314-f821adc9f9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ICUSTAYS Columns:\n",
            "Index(['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'dbsource',\n",
            "       'first_careunit', 'last_careunit', 'first_wardid', 'last_wardid',\n",
            "       'intime', 'outtime', 'los'],\n",
            "      dtype='object')\n",
            "   row_id  subject_id  hadm_id  icustay_id dbsource first_careunit  \\\n",
            "0       1           2   163353      243653  carevue           NICU   \n",
            "1       2           3   145834      211552  carevue           MICU   \n",
            "2       4           5   178980      214757  carevue           NICU   \n",
            "3       6           7   118037      278444  carevue           NICU   \n",
            "4       7           7   118037      236754  carevue           NICU   \n",
            "\n",
            "  last_careunit  first_wardid  last_wardid               intime  \\\n",
            "0          NICU            56           56  2138-07-17 21:20:07   \n",
            "1          MICU            12           12  2101-10-20 19:10:11   \n",
            "2          NICU            56           56  2103-02-02 06:04:24   \n",
            "3          NICU            56           56  2121-05-23 15:35:29   \n",
            "4          NICU            56           56  2121-05-25 03:26:01   \n",
            "\n",
            "               outtime     los  \n",
            "0  2138-07-17 23:32:21  0.0918  \n",
            "1  2101-10-26 20:43:09  6.0646  \n",
            "2  2103-02-02 08:06:00  0.0844  \n",
            "3  2121-05-23 22:01:00  0.2677  \n",
            "4  2121-05-25 21:10:19  0.7391  \n",
            "ADMISSIONS Columns:\n",
            "Index(['row_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime',\n",
            "       'deathtime', 'admission_type', 'admission_location',\n",
            "       'discharge_location', 'insurance', 'language', 'religion',\n",
            "       'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'diagnosis',\n",
            "       'hospital_expire_flag', 'has_chartevents_data'],\n",
            "      dtype='object')\n",
            "   row_id  subject_id  hadm_id            admittime            dischtime  \\\n",
            "0       1           2   163353  2138-07-17 19:04:00  2138-07-21 15:48:00   \n",
            "1       2           3   145834  2101-10-20 19:08:00  2101-10-31 13:58:00   \n",
            "2       4           5   178980  2103-02-02 04:31:00  2103-02-04 12:15:00   \n",
            "3       6           7   118037  2121-05-23 15:05:00  2121-05-27 11:57:00   \n",
            "4       7           8   159514  2117-11-20 10:22:00  2117-11-24 14:20:00   \n",
            "\n",
            "  deathtime admission_type         admission_location discharge_location  \\\n",
            "0       NaN        NEWBORN  PHYS REFERRAL/NORMAL DELI               HOME   \n",
            "1       NaN      EMERGENCY       EMERGENCY ROOM ADMIT                SNF   \n",
            "2       NaN        NEWBORN  PHYS REFERRAL/NORMAL DELI               HOME   \n",
            "3       NaN        NEWBORN  PHYS REFERRAL/NORMAL DELI               HOME   \n",
            "4       NaN        NEWBORN  PHYS REFERRAL/NORMAL DELI               HOME   \n",
            "\n",
            "  insurance language       religion marital_status ethnicity  \\\n",
            "0   Private      NaN  NOT SPECIFIED            NaN     ASIAN   \n",
            "1  Medicare      NaN       CATHOLIC        MARRIED     WHITE   \n",
            "2   Private      NaN       BUDDHIST            NaN     ASIAN   \n",
            "3   Private      NaN       CATHOLIC            NaN     WHITE   \n",
            "4   Private      NaN       CATHOLIC            NaN     WHITE   \n",
            "\n",
            "             edregtime            edouttime    diagnosis  \\\n",
            "0                  NaN                  NaN      NEWBORN   \n",
            "1  2101-10-20 17:09:00  2101-10-20 19:24:00  HYPOTENSION   \n",
            "2                  NaN                  NaN      NEWBORN   \n",
            "3                  NaN                  NaN      NEWBORN   \n",
            "4                  NaN                  NaN      NEWBORN   \n",
            "\n",
            "   hospital_expire_flag  has_chartevents_data  \n",
            "0                     0                     1  \n",
            "1                     0                     1  \n",
            "2                     0                     1  \n",
            "3                     0                     1  \n",
            "4                     0                     1  \n",
            "NOTEEVENTS Columns:\n",
            "Index(['row_id', 'subject_id', 'hadm_id', 'chartdate', 'charttime',\n",
            "       'storetime', 'category', 'description', 'cgid', 'iserror', 'text'],\n",
            "      dtype='object')\n",
            "   row_id  subject_id  hadm_id            chartdate            charttime  \\\n",
            "0  738407       20409      NaN  2119-01-04 00:00:00  2119-01-04 12:59:00   \n",
            "1  738408       20409      NaN  2119-01-09 00:00:00  2119-01-09 13:05:00   \n",
            "2  738409       20409      NaN  2119-01-16 00:00:00  2119-01-16 21:24:00   \n",
            "3  738410       20409      NaN  2119-01-18 00:00:00  2119-01-18 13:24:00   \n",
            "4  738411       20409      NaN  2119-01-18 00:00:00  2119-01-18 15:45:00   \n",
            "\n",
            "  storetime   category                        description  cgid  iserror  \\\n",
            "0       NaN  Radiology      ABDOMEN U.S. (COMPLETE STUDY)   NaN      NaN   \n",
            "1       NaN  Radiology             MR LIVER WITH CONTRAST   NaN      NaN   \n",
            "2       NaN  Radiology                CHEST (PORTABLE AP)   NaN      NaN   \n",
            "3       NaN  Radiology                     CT ABD W&W/O C   NaN      NaN   \n",
            "4       NaN  Radiology  PARACENTESIS DIAG. OR THERAPEUTIC   NaN      NaN   \n",
            "\n",
            "                                                text  \n",
            "0  [**2119-1-4**] 12:59 PM\\n ABDOMEN U.S. (COMPLE...  \n",
            "1  [**2119-1-9**] 1:05 PM\\n MR LIVER WITH CONTRAS...  \n",
            "2  [**2119-1-16**] 9:24 PM\\n CHEST (PORTABLE AP) ...  \n",
            "3  [**2119-1-18**] 1:24 PM\\n CT ABD W&W/O C; CT P...  \n",
            "4  [**2119-1-18**] 3:45 PM\\n PARACENTESIS DIAG. O...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ICUSTAYS dataset\n",
        "# icustays_df = pd.read_csv('/content/drive/My Drive/MIMICfull/ICUSTAYS.csv')\n",
        "num_records_icustays = icustays_df.shape[0]\n",
        "print(f\"Number of records loaded for icustays_df: {num_records_icustays}\")\n",
        "\n",
        "# Load ADMISSIONS dataset\n",
        "# admissions_df = pd.read_csv('/content/drive/My Drive/MIMICfull/ADMISSIONS.csv')\n",
        "num_records_admissions = admissions_df.shape[0]\n",
        "print(f\"Number of records loaded for admissions_df: {num_records_admissions}\")\n",
        "\n",
        "# Step 2: Merge ICUSTAYS with ADMISSIONS on subject_id and hadm_id\n",
        "merged_icustays_admissions = pd.merge(icustays_df, admissions_df, on=['subject_id', 'hadm_id'], how='inner')\n",
        "num_records_merged_icustays_admissions = merged_icustays_admissions.shape[0]\n",
        "print(f\"Number of records loaded for merged_icustays_admissions: {num_records_merged_icustays_admissions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp6sbfg6GhwA",
        "outputId": "2150ba12-9e80-4983-c10f-f44ed828dc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records loaded for icustays_df: 28391\n",
            "Number of records loaded for admissions_df: 26836\n",
            "Number of records loaded for merged_icustays_admissions: 28391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Filter CHARTEVENTS for Relevant ITEMIDs and Merge\n",
        "\n",
        "# Load and filter CHARTEVENTS dataset more efficiently\n",
        "filtered_chartevents = pd.read_csv('/content/drive/My Drive/MIMICsubset/CHARTEVENTS.csv', usecols=['subject_id', 'icustay_id', 'itemid', 'charttime', 'valuenum'])\n",
        "num_records_filtered_chartevents = filtered_chartevents.shape[0]\n",
        "print(f\"Number of records loaded for admissions_df: {num_records_filtered_chartevents}\")\n",
        "\n",
        "# Ensure that subject_id and icustay_id are of the same type in both DataFrames\n",
        "merged_icustays_admissions['subject_id'] = merged_icustays_admissions['subject_id'].astype(int)\n",
        "merged_icustays_admissions['icustay_id'] = merged_icustays_admissions['icustay_id'].astype(float)\n",
        "filtered_chartevents['subject_id'] = filtered_chartevents['subject_id'].astype(int)\n",
        "filtered_chartevents['icustay_id'] = filtered_chartevents['icustay_id'].astype(float)\n",
        "\n",
        "# Merge the filtered CHARTEVENTS with merged_icustays_admissions on subject_id and icustay_id\n",
        "merged_with_chartevents = pd.merge(merged_icustays_admissions, filtered_chartevents, on=['subject_id', 'icustay_id'], how='inner')\n",
        "num_records_merged_with_chartevents = merged_with_chartevents.shape[0]\n",
        "print(f\"Number of records loaded for merged_with_chartevents: {num_records_merged_with_chartevents}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqfhnxmXGuo0",
        "outputId": "41631d22-27ea-4365-9ed9-043f59612d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records loaded for admissions_df: 758355\n",
            "Number of records loaded for merged_with_chartevents: 359882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.1: Aggregate Vital Signs\n",
        "# Aggregate vital sign values for each ICU stay (e.g., average and max values)\n",
        "aggregated_vitals = merged_with_chartevents.groupby('icustay_id').agg(\n",
        "    mean_valuenum=('valuenum', 'mean'),\n",
        "    max_valuenum=('valuenum', 'max'),\n",
        "    min_valuenum=('valuenum', 'min')\n",
        ").reset_index()\n",
        "print(\"Aggregated Vital Signs:\")\n",
        "print(aggregated_vitals.head())\n",
        "\n",
        "num_records_aggregated_vitals = aggregated_vitals.shape[0]\n",
        "print(f\"Number of records loaded for aggregated_vitals: {num_records_aggregated_vitals}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJqqYAbdM8jD",
        "outputId": "a0fbd72e-cf29-47ac-d463-bd4094855120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated Vital Signs:\n",
            "   icustay_id  mean_valuenum  max_valuenum  min_valuenum\n",
            "0    201006.0      73.526690   2000.000000         -10.0\n",
            "1    203766.0      87.808804   5948.720215         -17.0\n",
            "2    204201.0      52.911634    285.000000           0.4\n",
            "3    204881.0      64.937683    270.000000           0.3\n",
            "4    206504.0      63.693601    246.000000           1.0\n",
            "Number of records loaded for aggregated_vitals: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a list of keywords related to delirium that we want to identify in clinical notes\n",
        "delirium_keywords = [\"delirium\", \"confusion\", \"disorientation\", \"agitation\", \"hallucination\", \"altered mental status\"]\n",
        "\n",
        "# Create a function to check if any delirium-related keywords are present in the text\n",
        "def identify_delirium(text):\n",
        "    if isinstance(text, str):\n",
        "        for keyword in delirium_keywords:\n",
        "            if re.search(r'\\b' + keyword + r'\\b', text, re.IGNORECASE):\n",
        "                return 1  # Label as 1 if any keyword is found\n",
        "    return 0  # Label as 0 otherwise\n",
        "\n",
        "# Apply the function to create a 'delirium_label' column\n",
        "noteevents_df['delirium_label'] = noteevents_df['text'].apply(identify_delirium)\n",
        "#print(\"NOTEEVENTS with Delirium Label:\")\n",
        "#print(noteevents_df[['subject_id', 'hadm_id', 'text', 'delirium_label']].head())\n",
        "\n",
        "num_records_noteevents_df = noteevents_df.shape[0]\n",
        "print(f\"Number of records loaded for noteevents_df: {num_records_noteevents_df}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnJXC8NcNE99",
        "outputId": "28f2b9ee-04f5-4e59-e0b0-4cf07a9dc27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records loaded for noteevents_df: 880107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.3: Merge Aggregated Vitals and Clinical Notes\n",
        "# Merge the aggregated vitals with the merged ICUSTAYS and ADMISSIONS dataframe\n",
        "merged_features = pd.merge(merged_icustays_admissions, aggregated_vitals, on='icustay_id', how='left')\n",
        "\n",
        "# Merge with NOTEEVENTS to include delirium labels\n",
        "final_merged_df = pd.merge(merged_features, noteevents_df[['subject_id', 'hadm_id', 'delirium_label']], on=['subject_id', 'hadm_id'], how='left')\n",
        "\n",
        "num_records_final_merged_df = final_merged_df.shape[0]\n",
        "print(f\"Number of records loaded for final_merged_df: {num_records_final_merged_df}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU7HmI3KNL2h",
        "outputId": "78f8aeb8-4f27-4f2e-f85e-fb96a957a992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records loaded for final_merged_df: 952027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Data Cleaning\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming final_merged_df is properly defined as a DataFrame above this code\n",
        "\n",
        "# Step 5.1: Impute missing vital signs with their respective means\n",
        "final_merged_df.loc[:, 'mean_valuenum'] = final_merged_df['mean_valuenum'].fillna(final_merged_df['mean_valuenum'].mean())\n",
        "final_merged_df.loc[:, 'max_valuenum'] = final_merged_df['max_valuenum'].fillna(final_merged_df['max_valuenum'].mean())\n",
        "final_merged_df.loc[:, 'min_valuenum'] = final_merged_df['min_valuenum'].fillna(final_merged_df['min_valuenum'].mean())\n",
        "\n",
        "# Step 5.2: Impute missing delirium labels with 0 (assuming no delirium)\n",
        "final_merged_df.loc[:, 'delirium_label'] = final_merged_df['delirium_label'].fillna(0)\n",
        "\n",
        "# Step 5.3: Impute categorical missing values with 'Unknown'\n",
        "final_merged_df.loc[:, 'language'] = final_merged_df['language'].fillna('Unknown')\n",
        "final_merged_df.loc[:, 'religion'] = final_merged_df['religion'].fillna('Unknown')\n",
        "final_merged_df.loc[:, 'marital_status'] = final_merged_df['marital_status'].fillna('Unknown')\n",
        "\n",
        "# Step 5.4: Fill missing deathtime values with 0 (indicating survival)\n",
        "final_merged_df.loc[:, 'deathtime'] = final_merged_df['deathtime'].fillna(0)\n",
        "\n",
        "# Step 5.5: Drop rows with missing 'outtime' or 'los' if they exist in DataFrame\n",
        "missing_columns = [col for col in ['outtime', 'los'] if col in final_merged_df.columns]\n",
        "if missing_columns:\n",
        "    final_merged_df = final_merged_df.dropna(subset=missing_columns)\n",
        "\n",
        "# Step 5.6: Impute missing 'diagnosis' values with 'Unknown'\n",
        "final_merged_df.loc[:, 'diagnosis'] = final_merged_df['diagnosis'].fillna('Unknown')\n",
        "\n",
        "# Step 5.7: Dropping unnecessary columns if they exist in DataFrame\n",
        "unnecessary_columns = [col for col in ['edregtime', 'edouttime'] if col in final_merged_df.columns]\n",
        "if unnecessary_columns:\n",
        "    final_merged_df = final_merged_df.drop(columns=unnecessary_columns)\n",
        "\n",
        "# Ensuring final_merged_df is still a DataFrame and not accidentally assigned to a method\n",
        "assert isinstance(final_merged_df, pd.DataFrame), \"final_merged_df must be a DataFrame\"\n",
        "\n",
        "print(final_merged_df.info())\n",
        "print(final_merged_df.head())\n",
        "\n",
        "delirium_count = final_merged_df[final_merged_df['delirium_label'] == 1].shape[0]\n",
        "print(f\"Number of rows with delirium = 1: {delirium_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFPSaUDdThXI",
        "outputId": "eacea22f-175c-460e-8bbf-47a451931cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 951647 entries, 0 to 952026\n",
            "Data columns (total 31 columns):\n",
            " #   Column                Non-Null Count   Dtype  \n",
            "---  ------                --------------   -----  \n",
            " 0   row_id_x              951647 non-null  int64  \n",
            " 1   subject_id            951647 non-null  int64  \n",
            " 2   hadm_id               951647 non-null  int64  \n",
            " 3   icustay_id            951647 non-null  float64\n",
            " 4   dbsource              951647 non-null  object \n",
            " 5   first_careunit        951647 non-null  object \n",
            " 6   last_careunit         951647 non-null  object \n",
            " 7   first_wardid          951647 non-null  int64  \n",
            " 8   last_wardid           951647 non-null  int64  \n",
            " 9   intime                951647 non-null  object \n",
            " 10  outtime               951647 non-null  object \n",
            " 11  los                   951647 non-null  float64\n",
            " 12  row_id_y              951647 non-null  int64  \n",
            " 13  admittime             951647 non-null  object \n",
            " 14  dischtime             951647 non-null  object \n",
            " 15  deathtime             951647 non-null  object \n",
            " 16  admission_type        951647 non-null  object \n",
            " 17  admission_location    951647 non-null  object \n",
            " 18  discharge_location    951647 non-null  object \n",
            " 19  insurance             951647 non-null  object \n",
            " 20  language              951647 non-null  object \n",
            " 21  religion              951647 non-null  object \n",
            " 22  marital_status        951647 non-null  object \n",
            " 23  ethnicity             951647 non-null  object \n",
            " 24  diagnosis             951647 non-null  object \n",
            " 25  hospital_expire_flag  951647 non-null  int64  \n",
            " 26  has_chartevents_data  951647 non-null  int64  \n",
            " 27  mean_valuenum         951647 non-null  float64\n",
            " 28  max_valuenum          951647 non-null  float64\n",
            " 29  min_valuenum          951647 non-null  float64\n",
            " 30  delirium_label        951647 non-null  float64\n",
            "dtypes: float64(6), int64(8), object(17)\n",
            "memory usage: 232.3+ MB\n",
            "None\n",
            "   row_id_x  subject_id  hadm_id  icustay_id dbsource first_careunit  \\\n",
            "0         1           2   163353    243653.0  carevue           NICU   \n",
            "1         1           2   163353    243653.0  carevue           NICU   \n",
            "2         2           3   145834    211552.0  carevue           MICU   \n",
            "3         2           3   145834    211552.0  carevue           MICU   \n",
            "4         2           3   145834    211552.0  carevue           MICU   \n",
            "\n",
            "  last_careunit  first_wardid  last_wardid               intime  ...  \\\n",
            "0          NICU            56           56  2138-07-17 21:20:07  ...   \n",
            "1          NICU            56           56  2138-07-17 21:20:07  ...   \n",
            "2          MICU            12           12  2101-10-20 19:10:11  ...   \n",
            "3          MICU            12           12  2101-10-20 19:10:11  ...   \n",
            "4          MICU            12           12  2101-10-20 19:10:11  ...   \n",
            "\n",
            "        religion  marital_status  ethnicity    diagnosis hospital_expire_flag  \\\n",
            "0  NOT SPECIFIED         Unknown      ASIAN      NEWBORN                    0   \n",
            "1  NOT SPECIFIED         Unknown      ASIAN      NEWBORN                    0   \n",
            "2       CATHOLIC         MARRIED      WHITE  HYPOTENSION                    0   \n",
            "3       CATHOLIC         MARRIED      WHITE  HYPOTENSION                    0   \n",
            "4       CATHOLIC         MARRIED      WHITE  HYPOTENSION                    0   \n",
            "\n",
            "  has_chartevents_data mean_valuenum max_valuenum min_valuenum delirium_label  \n",
            "0                    1      76.46329  6089.315439   -75.482551            0.0  \n",
            "1                    1      76.46329  6089.315439   -75.482551            0.0  \n",
            "2                    1      76.46329  6089.315439   -75.482551            0.0  \n",
            "3                    1      76.46329  6089.315439   -75.482551            0.0  \n",
            "4                    1      76.46329  6089.315439   -75.482551            0.0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "Number of rows with delirium = 1: 20819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 6: Extract Features for Modeling\n",
        "# Extract relevant features for modeling, including the delirium label\n",
        "modeling_features_df = final_merged_df[['icustay_id', 'subject_id', 'mean_valuenum', 'max_valuenum', 'min_valuenum', 'delirium_label']]\n",
        "\n",
        "# Drop duplicates to ensure no redundant data\n",
        "modeling_features_df = modeling_features_df.drop_duplicates(subset=['icustay_id'])\n",
        "\n",
        "# Step 7: Data Splitting for Model Training\n",
        "# Define features (X) and target (y)\n",
        "X = modeling_features_df.drop(columns=['subject_id', 'icustay_id', 'delirium_label'])\n",
        "y = modeling_features_df['delirium_label']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Model Training - Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Model Evaluation\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYmJLGc6bf8N",
        "outputId": "a7b0df70-60ee-4fee-b8de-a826369c6e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.01620574246961423\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.00      0.00      5593\n",
            "         1.0       0.01      0.99      0.03        84\n",
            "\n",
            "    accuracy                           0.02      5677\n",
            "   macro avg       0.46      0.49      0.02      5677\n",
            "weighted avg       0.89      0.02      0.00      5677\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SwUiIELssSU",
        "outputId": "f882ee48-1d9c-4dd1-d67b-31dff7a41d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: imbalanced-learn==0.10.1 in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Step 6: Extract Features for Modeling\n",
        "# Extract relevant features for modeling, including the delirium label\n",
        "modeling_features_df = final_merged_df[['icustay_id', 'subject_id', 'mean_valuenum', 'max_valuenum', 'min_valuenum', 'delirium_label']]\n",
        "\n",
        "# Drop duplicates to ensure no redundant data\n",
        "modeling_features_df = modeling_features_df.drop_duplicates(subset=['icustay_id'])\n",
        "\n",
        "# Step 7: Data Splitting for Model Training\n",
        "# Define features (X) and target (y)\n",
        "X = modeling_features_df.drop(columns=['subject_id', 'icustay_id', 'delirium_label'])\n",
        "y = modeling_features_df['delirium_label']\n",
        "\n",
        "# Step 7.1: Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the resampled data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Model Training - Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Model Evaluation\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nlkDCd7dywS",
        "outputId": "110abce5-9c31-4ad7-d765-46eb3cbcedaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.5001340842048807\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      1.00      0.67      5577\n",
            "         1.0       0.95      0.00      0.01      5610\n",
            "\n",
            "    accuracy                           0.50     11187\n",
            "   macro avg       0.72      0.50      0.34     11187\n",
            "weighted avg       0.73      0.50      0.34     11187\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfenWeihs3E-",
        "outputId": "f1438c1c-ba21-4220-9b23-57c8fd788cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.23.4 xgboost-2.1.2\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Step 6: Extract Features for Modeling\n",
        "# Use all available features except 'subject_id' and 'icustay_id' as they are identifiers\n",
        "modeling_features_df = final_merged_df.drop(columns=['subject_id', 'icustay_id'])\n",
        "\n",
        "# Drop duplicates to ensure no redundant data\n",
        "modeling_features_df = modeling_features_df.drop_duplicates()\n",
        "\n",
        "# Step 7: Encoding Categorical Features\n",
        "# Convert categorical columns to numeric using one-hot encoding\n",
        "modeling_features_df = pd.get_dummies(modeling_features_df, drop_first=True)\n",
        "\n",
        "# Convert numeric columns to float32 to save memory\n",
        "for col in modeling_features_df.select_dtypes(include=['float64']).columns:\n",
        "    modeling_features_df[col] = modeling_features_df[col].astype('float32')\n",
        "\n",
        "# Step 8: Data Splitting for Model Training\n",
        "X = modeling_features_df.drop(columns=['delirium_label'])\n",
        "y = modeling_features_df['delirium_label']\n",
        "\n",
        "# Feature Selection to reduce dimensionality\n",
        "k = 20  # Choose a suitable number of features based on available resources\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X = selector.fit_transform(X, y)\n",
        "\n",
        "# Applying SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 9: Hyperparameter Tuning - XGBoost Classifier\n",
        "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 6, 10],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV to reduce memory usage\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=10, cv=3, n_jobs=-1, verbose=0, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters from random search\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Step 10: Model Evaluation\n",
        "# Train the best model with the entire training set\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoiYVqWVoEPT",
        "outputId": "c9738e98-b449-4b07-fbad-088ea288f068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Best Parameters: {'subsample': 1.0, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
            "Model Accuracy: 0.7722545390445972\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.83      0.79      5669\n",
            "         1.0       0.81      0.71      0.76      5677\n",
            "\n",
            "    accuracy                           0.77     11346\n",
            "   macro avg       0.78      0.77      0.77     11346\n",
            "weighted avg       0.78      0.77      0.77     11346\n",
            "\n"
          ]
        }
      ]
    }
  ]
}